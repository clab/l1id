\documentclass[11pt,letterpaper]{article}
\usepackage[linkcolor=blue]{hyperref}	% must precede ACL style
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{tipa}


\usepackage{amsmath,amsfonts,eucal,amsbsy,amsthm,amsopn,amssymb}

\usepackage{txfonts}
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}

\usepackage{paralist}
\defaultleftmargin{1em}{}{}{} % default is 2em

\usepackage{array}

\usepackage[round]{natbib}
\setlength\titlebox{6.5cm}    % Expanding the titlebox



% Author comments
\usepackage{color}
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{purple}{rgb}{0.7,0,1.0}
\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\swmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{S}}_{\textsc{W}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ntmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{N}}_{\textsc{T}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\sw}[1]{\arkcomment{\swmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\nt}[1]{\arkcomment{\ntmarker}{#1}{orange}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\cost}{c}

\newcommand{\Sref}[1]{\S\ref{#1}}
\newcommand{\fref}[1]{figure~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{table~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}


% special macros

\newcommand{\feat}[1]{\textsmaller[.5]{\textsf{#1}}} % code for a feature group
\newcommand{\textnl}{\textsl}



\title{Identifying the L1 of non-native writers: the CMU-Haifa system\\[1em]
{\large Chris Dyer$^\ast$ Manaal Faruqui$^\ast$ Noam Ordan$^\dagger$ Nathan Schneider$^\ast$\\ Yulia Tsvetkov$^\ast$ Naama Twitto$^\dagger$ Shuly Wintner$^\dagger$}\\[-3em]
}

\author{
\\
$^\ast$Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
\And
\\
$^\dagger$Department of Computer Science\\University of Haifa\\Haifa, Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%C.\ Dyer M.\ Faruqui N.\ Schneider Y.\ Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%N.\ Ordan N.\ Twitto S.\ Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%Chris Dyer\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%Manaal Faruqui\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{manaalfar@gmail.com}
%\And
%Noam Ordan\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{noam.ordan@gmail.com}
%\And
%Nathan Schneider\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{nathan@cmu.edu}
%\And
%Yulia Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{yulia.tsvetkov@gmail.com}
%\And
%Naama Twitto\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{naama.twitto@gmail.com}
%\And
%Shuly Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Given a dataset of English essays composed by non-native speakers, as
part of the TOEFL exam, we identify with high accuracy the native
language of the authors. We use standard text classification
techniques, but define sophisticated classifiers that are sensitive to
the specific patterns observed in the English of authors whose first
language is structurally different. We describe the various features
used for classification, as well as the settings of the classifier
that yielded the highest accuracy.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The task we address in this work is identifying the native language
(\emph{L1}) of non-native English (\emph{L2}) authors. More specifically, given a
dataset of short English essays
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR},
composed as part of the \emph{Test of English as a Foreign Language
  (TOEFL)} by authors whose native language is one out of~11 possible
languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu, and Turkish), our task is to identify that
language.

This task has a clear empirical motivation. Non-native speakers make
different errors when they write English, depending on their native
language \citep{swan2001learner}; understanding the different types of
errors is a prerequisite for correcting them
\citep{Leacock:2010:AGE:1855062}, and systems such as the one we
describe here can shed interesting light on such errors. Tutoring
applications can use our system to identify the native language of
students and offer better-targeted advice. Forensic linguistic
applications are sometimes required to determine the L1 of authors
\citep{estival-gaustad-pham-radford-hutchinson:2007:ALTA2007,estival2007author}. Additionally,
we believe that the task is interesting in and of itself, providing a
better understanding of non-native language. We are thus equally
interested in defining \emph{meaningful} features whose contribution
to the task can be linguistically interpreted.

We address the task as a multiway text-classification task; we specify
our methodology in \Sref{sec:methodology}. As in other author
attribution tasks \citep{joula:2006}, the choice of features for the
classifier is crucial; we discuss the features we define in
\Sref{sec:features}. We report our results in
\Sref{sec:results} and conclude with suggestions for future
research.

\section{Related work}
\label{sec:literature}
The task of L1 identification was introduced by
\citet{koppel2005automatically,koppel2005determining}, who work on the
International Corpus of Learner English \citep{icle}, which includes
texts written by students from Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts lengths range from 500 to 850 words. The
classification method is a linear SVM, and features include 400
standard function words, 200 letter $n$-grams, 185 error types and 250
rare part-of-speech (POS) bi-grams. Ten-fold cross-validation results
on this dataset are 80\% accuracy.

The same experimental setup is assumed by
\citet{tsur-rappoport:2007:Cognitive-2007}, who are mostly interested
in testing the hypothesis that an author's choice of words in a second
language is influenced by the phonology of his or her L1. They confirm
this hypothesis by carefully analyzing the features used by
\citeauthor{koppel2005automatically}, controlling for potential
biases.

\citet{Wong-Dras:2009:ALTA2009,wong-dras:2011:EMNLP} are also
motivated by a linguistic hypothesis, namely that syntactic errors in
a text are influenced by the author's
L1. \citet{Wong-Dras:2009:ALTA2009} analyze three error types
statistically, and then add them as features in the same experimental
setup as above (using LIBSVM with a radial kernel for
classification). The error types are subject-verb disagreement,
noun-number disagreement and misuse of determiners. Addition of these
features does not improve on the results of
\citeauthor{koppel2005automatically}. \citet{wong-dras:2011:EMNLP}
further extend this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic structure. This improves
the results significantly, yielding 78\% accuracy compared with less
than 65\% using only lexical features.

\citet{kochmar2011identification} uses a different corpus, the
Cambridge Learner Corpus, in which texts are 200-400 word long, and
are authored by native speakers of five Germanic languages (German,
Swiss German, Dutch, Swedish and Danish) and five Romance languages
(French, Italian, Catalan, Spanish and Portuguese). Again, SVM is the
classification device. Features include POS $n$-grams, character
$n$-grams, phrase-structure rules (extracted from parse trees), and
two measures of error rate. The classifier is evaluated on its ability
to distinguish between pairs of closely-related L1s, and the results
are usually excellent.

A completely different approach is offered by
\citet{brooke2011native}. Since training corpora for this task are
rare, they use mainly L1 (blog) corpora. Given English word bi-grams
$\langle e_1,e_2\rangle$, they try to assess, for each L1, how likely
it is that an L1 bi-gram was translated literally by the author,
resulting in $\langle e_1,e_2\rangle$. Working with four L1s (French,
Spanish, Chinese, and Japanese), and evaluating on the International
Corpus of Learner English, the results are lower than 50\%.

Our dataset in this work is different, and consists of TOEFL essays
written by speakers of eleven different L1s
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR},
distributed as part of the First Native Language Identification Shared
Task \citep{tetreault-blanchard-cahill:2013:BEA}. We use a plethora of
features; some of them are inspired by previous work outlined above,
but many are motivated by other author attribution tasks, in
particular identification of \emph{translationese}, the language of
translated texts \citep{vered:noam:shuly}.

\section{Methodology}
\label{sec:methodology}
The dataset released by the NLI Shared task \citep{tetreault-EtAl:2012:PAPERS} is a balanced set of 1000 TOEFL essays per each native language. Essays are short, consisting of 10 to 20 sentences each. The dataset is split into 900 documents for training and 100 for development. Each document is annotated with the author's English proficiency level (low, medium, high) and an identification (1 to 8) of the prompt given to the author to trigger the essay. All essays are tokenized and split into sentences. We detail in \Tref{tbl:stats:level} some statistics on the training corpora, listed by the authors' proficiency level.

\begin{table}[hbt]
\small\centering
\begin{tabular}{lrrr}
% \textbf{Proficiency}  & \textbf{\# Documents} & \textbf{\# Tokens} & \textbf{\# Types} \\
%\hline
%Low & 1.069 & 245,130 & 13,110 \\
%Medium & 5,366 & 1,819,407 & 37,393 \\ 
%High & 3,456 & 1,388,260 & 28,329 \\ 
  & \multicolumn{1}{c}{\textbf{Low}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{High}} \\
\hline
\# Documents & 1,069 & 5,366 & 3,456 \\
\# Tokens & 245,130 & 1,819,407 & 1,388,260 \\ 
\# Types & 13,110 & 37,393 & 28,329 \\ 
\end{tabular}
\caption{Training set statistics.}
\label{tbl:stats:level}
\end{table} 

For classification we use the \texttt{creg} regression modeling framework to train a 11-class logistic regression classifier.\footnote{https://github.com/redpony/creg}  We parameterize the L1 classifier as a multiclass logistic regression:
\begin{align*}
p(y& \mid \textbf{d}, i) \propto \exp \sum_j \lambda_j h_j(y, \textbf{d}, i) ,
\end{align*}
where $\textbf{d}$ are documents, $h_j(\cdot)$ are feature functions, $\lambda_j$ are the corresponding weights, and $y$ refer to the outputs: eleven L1 class labels. \yt{regularization???}  



All essays were tagged with the Stanford part-of-speech tagger
\citep{toutanova-03}. We did not parse the dataset.

\section{Model Overview}
\label{sec:features}
We define a large arsenal of features, our motivation being both to
improve the accuracy of classification and to be able to interpret the
characteristics of the language produced by speakers of different
L1s.

\subsection{Motivation}
While some of the features were used in the works surveyed in
\Sref{sec:literature}, many are novel, and are inspired by the
features used to identify translationese by \citet{vered:noam:shuly}.
We begin by motivating our choice of features.

\begin{compactdesc}
\item[POS $n$-grams] Part-of-speech $n$-grams were used in various
  text-classification tasks.
\item[Prompt] Since the prompt contributes information on the domain,
  it is likely that some words (and, hence, character sequences) will
  occur more frequently with some prompts than with others. We
  therefore use the prompt ID in conjunction with other features.
\item[Document length] The number of tokens in the text is highly
  correlated with the author's level of fluency, which in turn is
  correlated with the author's L1.
\item[Pronouns] The use of pronouns varies greatly among different
  authors. We use the same list of~25 English pronouns that
  \citet{vered:noam:shuly} use for identifying translationese.
\item[Punctuation] Similarly, different languages use punctuation
  differently, and we expect this to taint the use of punctuation in
  non-native texts. Of course, character $n$-grams subsume this feature.
\item[Passives] English uses passive voice more frequently than other
  languages. Again, the use of passives in L2 can be correlated with
  the author's L1.
\item[Positional token frequency] The choice of the first and last few
  words in a sentence is highly constrained, and may be significantly
  influenced by the author's L1.
\item[Cohesive markers] These are 40 function words (and short
  phrases) that have a strong discourse function in texts (\textnl{however},
  \textnl{because}, \textnl{in fact}, etc.) Translators tend to spell out implicit
  utterances and render them explicitly in the target text
  \citep{Blum-Kulka:1986}. We use the list
  of~\citet{vered:noam:shuly}.
\item[Cohesive verbs] This is a list of manually compiled verbs that
  are used, like cohesive markers, to spell out implicit utterances
  (\textnl{indicate}, \textnl{imply}, \textnl{contain}, etc.)
\item[Function words] Frequent tokens, which are mostly function
  words, have been used successfully for various text classification
  tasks. \citet{koppel-ordan:2011:ACL-HLT2011} define a list of 400
  such words, of which we only use~100 (using the entire list was not
  significantly different). Note that pronouns are included in this list.
\item[Contextual function words] To further capitalize on the ability
  of function words to discriminate, we define pairs consisting of a
  function word from the list mentioned above, along with the POS tag
  of its adjacent word. This feature captures patterns such as verbs
  and the preposition or particle immediately to their right, or nouns
  and the determiner that precedes them. We also define 3-grams
  consisting of one or two function words and the POS tag of the third
  word in the 3-gram.
\item[Lemmas] The content of the text is not considered a good
  indication of the author's L1, but many text categorization tasks
  use lemmas (more precisely, the stems produced by the tagger) as
  features approximating the content.
\item[Misspelling features] Learning to perceive, produce, and encode non-native phonemic contrasts is extremely difficult for L2 learners \citep{hayes-harb:2008}. Since English's orthography is largely phonemic---even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse {\tt l} and {\tt r} relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector \citep{kernighan:1990} to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay.
\item[Restored tags] We focus on three important token classes defined
  above: punctuation marks, function words and cohesive verbs. We
  first remove words in these classes from the texts, and then recover
  the most likely hidden tokens in a sequence of words, according to
  an $n$-gram language model trained on all essays in the training
  corpus corrected with a spell checker and containing both words and
  hidden tokens. This feature should capture specific words or
  punctuation marks that are consistently omitted (deletions), or
  misused (insertions, substitutions). To restore hidden tokens we use
  the \texttt{hidden-ngram} utility provided in SRI's language modeling toolkit
  \citep{stolcke02srilm}.
\item[Brown clusters] \cite{brown:cl1992} describe an algorithm that induces a hierarchical clustering of a language's vocabulary based on each vocabulary item's tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in $n$-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text \citep{koo:2008,turian:acl2010,owoputi:2013}.  Using an open-source implementation of the algorithm,\footnote{\url{https://github.com/percyliang/brown-cluster}} we clustered 8~billion words of English into 600~classes.\footnote{\url{http://www.ark.cs.cmu.edu/cdyer/en-600/cluster_viewer.html}}  We included log counts of all $4$-grams of Brown clusters that occurred at least $100$ times in the NLI training data.

\end{compactdesc}

\subsection{Main Features}
\label{sec:mainfeats}
First, we use the following four feature types as the core of our
model.  Whenever counts are mentioned, we use the log of the count as
the feature.  We report the accuracy of using each feature type, in
isolation, on the training set.

\begin{compactdesc}
\item[\feat{POS}] Part-of-speech $n$-grams.  Features were extracted
  to count every POS 1-, 2-, 3- and 4-gram in each
  document. 55.18\%. 
\item[\feat{FreqChar}] Frequent character $n$-grams.  We experimented
  with character $n$-grams: The number of character 1-, 2-, and
  3-grams. This yielded 69.94\% accuracy.  We then refined the feature
  to include only those character $n$-grams that are observed more
  than $m$ times in the corpus are considered. $n$ ranges from~1
    to~4, and $m$ is set to~5. 74.12\%
\item[\feat{CharPrompt}] Conjunction of the character $n$-gram
  features defined above with the prompt ID. 65.09\%.
\item[\feat{Brown}] Substitutions, deletions and insertions counts of Brown clusters 1- and 2-grams in each document. 72.26\%.
\end{compactdesc}
\noindent
The accuracy of the classifier on the development set using these four
feature types is reported in \Tref{tbl:mainfeats}.

\begin{table}[hbt]
\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\feat{POS} & 540,947 & 55.18 & 1.0 \\
+ \feat{FreqChar} & 1,036,871 & 79.55 & 1.0 \\ 
\quad + \feat{CharPrompt} & 2,111,175 & 79.82 & 1.0 \\ 
\qquad + \feat{Brown} & 5,664,461 & 81.09 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} feature groups, added cumulatively. 
  The number of parameters is always a multiple of 11 (the number of classes). 
  Only $\ell_2$ regularization was used for these experiments; 
  the penalty was tuned on the dev set as well.}
\label{tbl:mainfeats}
\end{table}

\subsection{Additional Features}
To the basic set of features we now add more specific,
linguistically-motivated features, each adding a small number of
parameters to the model.  As above, we indicate the accuracy of each
feature type in isolation.

\begin{compactdesc}
\item[\feat{DocLen}] Document length in tokens. 11.81\%.
\item[\feat{Punct}] Counts of each punctuation mark. 27.41\%.
\item[\feat{Pron}] Counts of each pronoun. 22.81\%.
\item[\feat{Position}] Positional token frequency. We use the counts
  for the first two and last three words before the period in each
  sentence as features. 53.03\%.
\item[\feat{PsvRatio}] The proportion of passive verbs out of all
  verbs. 12.26\%.
\item[\feat{CxtFxn}] Contextual function words. Bi-grams yield
  62.79\%, tri-gram 62.32\%.
\item[\feat{Misspell}] Spelling correction edits. \sw{???}. 37.29\%.
\item[\feat{Restore}] Counts of substitutions, deletions and
  insertions of predefined tokens that we restored in the texts. 47.67\%
\end{compactdesc}
\noindent
\Tref{tbl:addfeats} reports the empirical improvement that each of
these brings independently when added to the main features
(\Sref{sec:mainfeats}).

\begin{table}[hbt]
\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\textsc{main} + \feat{Position} & 6,153,015 & 81.00 & 1.0 \\
\textsc{main} + \feat{PsvRatio} & 5,664,472 & 81.00 & 1.0 \\
\textsc{main} & 5,664,461 & 81.09 & 1.0 \\
\textsc{main} + \feat{DocLen} & 5,664,472 & 81.09 & 1.0 \\
\textsc{main} + \feat{Pron} & 5,664,736 & 81.09 & 1.0 \\
\textsc{main} + \feat{Punct} & 5,664,604 & 81.09 & 1.0 \\
\textsc{main} + \feat{Misspell} & 5,799,860 & 81.27 & 5.0 \\
\textsc{main} + \feat{Restore} & 5,682,589 & 81.36 & 5.0 \\
\textsc{main} + \feat{CxtFxn} & 7,669,684 & 81.73 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} features plus additional feature groups, added independently. 
$\ell_2$ regularization was tuned as in \Tref{tbl:mainfeats} (two values, 1.0 and 5.0, were tried for each 
configuration; more careful tuning might produce slightly better accuracy).
Results are sorted by accuracy; only three groups exhibited independent improvements over the \textsc{main} feature set.}
\label{tbl:addfeats}
\end{table}


\subsection{Discarded Features}
We also tried several other feature types that did not improve the
accuracy of the classifier on the development set.
\begin{compactdesc}
\item[Cohesive markers] Counts of each cohesive marker. 25.71\%.
\item[Cohesive verbs] Counts of each cohesive verb. 22.85\%.
\item[Function words] Counts of function words. 42.47\%. This feature
  is subsumed by the highly discriminative \feat{CxtFxn} feature.
\end{compactdesc}


\section{Results}
\label{sec:results}
The full model that we used to classify the test set combines all
features listed in \Tref{tbl:addfeats}. Using all these features, the
accuracy on the development set is~84.55\%, and on the test set it
is~81.5\%. \Tref{tbl:matrix} lists the confusion matrix on the test
set, as well as precision, recall and $F_1$-score for each L1.
The largest class of errors was predicting Telugu where the correct label 
was Hindi\nss{or vice versa?}---this happened 18 times. 



\begin{table*}[hbt]
\small\centering
\begin{tabular}{>{\bf}l|r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r|ccc} %from the email sent by the organizers
	& \bf ARA & \bf CHI & \bf FRE & \bf GER & \bf HIN & \bf ITA & \bf JPN & \bf KOR & \bf SPA & \bf TEL & \bf TUR & \bf Precision (\%) & \bf Recall (\%) & \bf $F_1$ (\%) \\
\hline
ARA & 80 & 0 & 2 & 1 & 3 & 4 & 1 & 0 & 4 & 2 & 3 & 80.8 & 80.0 & 80.4 \\
CHI & 3 & 80 & 0 & 1 & 1 & 0 & 6 & 7 & 1 & 0 & 1 & 88.9 & 80.0 & 84.2 \\
FRE & 2 & 2 & 81 & 5 & 1 & 2 & 1 & 0 & 3 & 0 & 3 & 86.2 & 81.0 & 83.5 \\
\hline
GER & 1 & 1 & 1 & 93 & 0 & 0 & 0 & 1 & 1 & 0 & 2 & 87.7 & 93.0 & 90.3 \\
HIN & 2 & 0 & 0 & 1 & 77 & 1 & 0 & 1 & 5 & 9 & 4 & 74.8 & 77.0 & 75.9 \\
ITA & 2 & 0 & 3 & 1 & 1 & 87 & 1 & 0 & 3 & 0 & 2 & 82.1 & 87.0 & 84.5 \\
\hline
JPN & 2 & 1 & 1 & 2 & 0 & 1 & 87 & 5 & 0 & 0 & 1 & 78.4 & 87.0 & 82.5 \\
KOR & 1 & 5 & 2 & 0 & 1 & 0 & 9 & 81 & 1 & 0 & 0 & 80.2 & 81.0 & 80.6 \\
SPA & 2 & 0 & 2 & 0 & 1 & 8 & 2 & 1 & 78 & 1 & 5 & 77.2 & 78.0 & 77.6 \\
\hline
TEL & 0 & 1 & 0 & 0 & 18 & 1 & 2 & 1 & 1 & 73 & 3 & 85.9 & 73.0 & 78.9 \\
TUR & 4 & 0 & 2 & 2 & 0 & 2 & 2 & 4 & 4 & 0 & 80 & 76.9 & 80.0 & 78.4 \\
\end{tabular}
\caption{Official test set confusion matrix with the full model. \nss{which direction is predicted vs. gold?}
Accuracy is 81.5\%.}
\label{tbl:matrix}
\end{table*}

Production of L2 texts, not unlike translating from L1 to L2, involves
a tension between the imposing models of L1 (and the source text), on
the one hand, and a set of cognitive constraints resulting from the
efforts to generate the target text, on the other. The former is
called \emph{interference} in Translation Studies \citep{Toury:1995}
and \emph{interlanguage} in second language acquisition
\citep{Selinker1972}.

\citet{vered:noam:shuly} designed 32 classifiers to test the validity
of the forces acting on translated texts, and found that interference
consistently yielded the best performing classifiers. And indeed, in
this work too, which replicates some of their classifiers, we find
again that fingerprints of the source language are dominant in the
makeup of L2 texts.

The main difference, however, between texts translated by
professionals and the texts we address here, is that more often than
not professional translators translate into their mother tongue,
whereas L2 writers write out of their mother tongue by definition. So
interference is ever more exaggerated in their case, for example, also
phonologically \citep{tsur-rappoport:2007:Cognitive-2007}.

We illustrate this with some examples from Arabic native speakers. The
character sequence \textnl{alot} is overrepresented in Arabic L2
texts. Arabic has no indefinite article and we speculate that Arabic
speakers conceive \textnl{a lot} as a single word; the Arabic
equivalent for \textnl{a lot} is used adverbially like an \textnl{-ly}
suffix in English. For the same reason, another prominent feature is a
missing definite article before nouns and adjectives . Additionally,
Arabic, being an Abjad language, rarely indicates vowels, and indeed
we find many missing \textnl{e}-s and \textnl{i}-s in the texts of
Arabic speakers. Phonologically, because Arabic conflates
/\textipa{I}/ and /\textipa{@}/ into /i/ (at least in Modern Standard
Arabic), we see that many \textnl{e}-s are indeed substituted for
\textnl{i}-s in these texts.

German overuses hyphens in two interesting ways. German can
notoriously use relative clauses freely, and such constructions
frequently occur between hyphens in the dataset, as in \textnl{any
  given rational being -- let us say Immanual Kant -- we find
  that}. Another overuse of hyphens stems from compounding, another
facet of German, for example in \textnl{well-known},
\textnl{community-help}, \textnl{spare-time}, \textnl{football-club},
etc. Many of these reflect an effort to both connect and separate
connected forms in the original (e.g., \textnl{Fussballklub}, which in
English would be more naturally rendered as \textnl{football club}).
Another unexpected feature of German is a frequent substitution of the
letter \textnl{y} for \textnl{z} (and vice versa), most probably
triggered by their switched positions on German keyboards.
 
The word \textnl{that} occurs more frequently in the texts of German
L1 speakers, perhaps because in English it is optional in relative
clauses whereas in German it is not.  Last, \textnl{often} is
overused; being a cognate of the German \textnl{oft} it is not
cognitively expensive to retrieve it. Spanish, on the other hand,
literally translates \textnl{muchas veces} into \textnl{many times},
which is similarly overused on the dataset. \sw{any Spanish speaker
  who could validate this expression?}

Other informative features include substitutions of \textnl{r}-s and
\textnl{l}-s in the texts of Japanese authors, for obvious reasons;
and the characters \textnl{r} and \textnl{s} are important in Chinese
and Spanish, respectively, for reasons that are unclear to
us. Similarly, the word \textnl{then} is dominant in the texts of
Hindi speakers. Finally, it is clear that authors refer to their
native cultures (and, consequently, native languages and countries);
the strings \textnl{Turkish}, \textnl{Korea}, and \textnl{Ita} were
dominant in the texts of Turkish, Korean and Italian native speakers,
respectively.

\section{Discussion}
\label{sec:discussion}

We experimented with different classifiers and a large set of features to solve an 11-way classification problem. Most of our features are linguistically informed and we believe that this direction should be further pursued. There are at least three motivations for identifying the mother tongue of L1 learners: (1) for the sake of sports, in which case linguistic and pedagogical theory doesn't matter: the more accurate the better; (2) to facilitate human assessment and grading; (3) to point out the differences between different L2 writers, on the one hand, and to discriminate L2 writers as a single set from native tongue writers, on the other; this can pave the way for improving teaching methods and identify the individual difficulties of specific L1 learners.

Our point of departure was the analogy between translated texts as a genre in its own and L2 writers as pseudo translators, relying heavily on their mother tongue and transferring their native models to a second language. Moreover, we assumed that like translators, L2 writers will write in a simplified manner and overuse explicit markers. Although this should be studied vis-a-vis comparable outputs of mother tongue writers in English, we observe that the best features of our classifiers are of the ``interference'' type, i.e. phonological, morphological and syntactic in nature, mostly in the form of misspelling features, restoration tags, punctuation and lexical and syntactic modeling.

We would like to stress that certain features indicating L2 do not have any bearing on the quality of the English produced, a phenomenon discussed extensively in Translation Studies \citep{Toury:1995}, where interference is observed by the overuse or underuse of certain features reflecting the typological differences between a specific pair of languages. For example, the fact that Italian native speakers favor the syntactic sequence of determiner + adjective + noun (e.g., \textnl{a big risk} or \textnl{this new business}) has little prescriptive value for teachers.

Last, we noted that certain L2 writers tend to repeat more often words appearing in the prompts, i.e., in the questions that triggered their essays. When we experimented on certain classifiers with and without the prompt number as a feature, we actually noted a decrease in performance. For the sake of sports, we should have in fact removed them from our classifier, but we stress their theoretical importance: some L2 writers repeat them more often and this may reflect different educational backgrounds (introducing this as a feature offsets its effect). Had we had a hypothetical 12th unseen L2, we believe it would have enabled us to cross-classify better. Again, this feature says nothing about the quality of the text, just as the tendency of Korean and Italian writers to mention their home country more often does not.
%\section{Conclusion}
%\label{sec:conclusion}

\section*{Acknowledgments}
This research was supported by a grant from the Israeli Ministry of
Science and Technology.\nss{anything from the CMU side?}

%\clearpage

\bibliography{l1id}
\bibliographystyle{plainnat}

\end{document}
