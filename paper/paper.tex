\documentclass[11pt,letterpaper]{article}
\usepackage[linkcolor=blue]{hyperref}	% must precede ACL style
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}


\usepackage{amsmath,amsfonts,eucal,amsbsy,amsthm,amsopn,amssymb}

\usepackage{txfonts}
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}

\usepackage{paralist}
\defaultleftmargin{1em}{}{}{} % default is 2em

\usepackage{array}

\usepackage[round]{natbib}
\setlength\titlebox{6.5cm}    % Expanding the titlebox



% Author comments
\usepackage{color}
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{purple}{rgb}{0.7,0,1.0}
\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\swmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{S}}_{\textsc{W}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ntmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{N}}_{\textsc{T}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\sw}[1]{\arkcomment{\swmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\nt}[1]{\arkcomment{\ntmarker}{#1}{orange}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\cost}{c}

\newcommand{\Sref}[1]{\S\ref{#1}}
\newcommand{\fref}[1]{figure~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{table~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}


% special macros

\newcommand{\feat}[1]{\textsmaller[.5]{\textsf{#1}}} % code for a feature group




\title{Identifying the L1 of non-native writers: the CMU-Haifa system\\[1em]
{\large Chris Dyer$^\ast$ Manaal Faruqui$^\ast$ Noam Ordan$^\dagger$ Nathan Schneider$^\ast$\\ Yulia Tsvetkov$^\ast$ Naama Twitto$^\dagger$ Shuly Wintner$^\dagger$}\\[-3em]
}

\author{
\\
$^\ast$Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
\And
\\
$^\dagger$Department of Computer Science\\University of Haifa\\Haifa, Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%C.\ Dyer M.\ Faruqui N.\ Schneider Y.\ Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%N.\ Ordan N.\ Twitto S.\ Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%Chris Dyer\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%Manaal Faruqui\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{manaalfar@gmail.com}
%\And
%Noam Ordan\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{noam.ordan@gmail.com}
%\And
%Nathan Schneider\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{nathan@cmu.edu}
%\And
%Yulia Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{yulia.tsvetkov@gmail.com}
%\And
%Naama Twitto\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{naama.twitto@gmail.com}
%\And
%Shuly Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Given a dataset of English essays composed by non-native speakers, as
part of the TOEFL exam, we identify with high accuracy the native
language of the authors. We use standard text classification
techniques, but define sophisticated classifiers that are sensitive to
the specific patterns observed in the English of authors whose first
language is structurally different. We describe the various features
used for classification, as well as the settings of the classifier
that yielded the highest accuracy.
\end{abstract}

\nss{shouldn't we use the official bib style file instead of natbib?}

\section{Introduction}
\label{sec:intro}
The task we address in this work is identifying the native language
(\emph{L1}) of non-native English authors. More specifically, given a
dataset
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR} of
short English essays, composed as part of the TOEFL exam (of English
as a foreign language) by authors whose native language is one out
of~11 possible languages (Arabic, Chinese, French, German, Hindi,
Italian, Japanese, Korean, Spanish, Telugu, and Turkish), our task is
to identify that language.

This task has a clear empirical motivation. Non-native speakers make
different errors when they write English, depending on their native
language \citep{swan2001learner}; understanding the different types of
errors is a prerequisite for correcting them
\citep{Leacock:2010:AGE:1855062}, and systems such as the one we
describe here can shed interesting light on such errors. Tutoring
applications can use our system to identify the native language of
students and offer better-targeted advice. Forensic linguistic
applications are sometimes required to determine the L1 of authors
\citep{estival-gaustad-pham-radford-hutchinson:2007:ALTA2007,estival2007author}. Additionally,
we believe that the task is interesting in and of itself, providing a
better understanding of non-native language. We are thus equally
interested in defining \emph{meaningful} features whose contribution
to the task can be linguistically interpreted.

We address the task as a multiway text-classification task; we specify
our methodology in \Sref{sec:methodology}. As in other author
attribution tasks \citep{joula:2006}, the choice of features for the
classifier is crucial; we discuss the features we define in
\Sref{sec:features}. We report our results in
\Sref{sec:results} and conclude with suggestions for future
research.

\section{Related work}
\label{sec:literature}
The task of L1 identification was introduced by
\citet{koppel2005automatically,koppel2005determining}, who work on the
International Corpus of Learner English \citep{icle}, which includes
texts written by students from Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts lengths range from 500 to 850 words. The
classification method is a linear SVM, and features include 400
standard function words, 200 letter $n$-grams, 185 error types and 250
rare part-of-speech (POS) bi-grams. Ten-fold cross-validation results
on this dataset are 80\% accuracy.

The same experimental setup is assumed by
\citet{tsur-rappoport:2007:Cognitive-2007}, who are mostly interested
in testing the hypothesis that an author's choice of words in a second
language is influenced by the phonology of his or her L1. They confirm
this hypothesis by carefully analyzing the features used by
\citeauthor{koppel2005automatically}, controlling for potential
biases.

\citet{Wong-Dras:2009:ALTA2009,wong-dras:2011:EMNLP} are also
motivated by a linguistic hypothesis, namely that syntactic errors in
a text are influenced by the author's
L1. \citet{Wong-Dras:2009:ALTA2009} analyze three error types
statistically, and then add them as features in the same experimental
setup as above (using LIBSVM with a radial kernel for
classification). The error types are subject-verb disagreement,
noun-number disagreement and misuse of determiners. Addition of these
features does not improve on the results of
\citeauthor{koppel2005automatically}. \citet{wong-dras:2011:EMNLP}
further extend this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic structure. This improves
the results significantly, yielding 78\% accuracy compared with less
than 65\% using only lexical features.

\citet{kochmar2011identification} uses a different corpus, the
Cambridge Learner Corpus, in which texts are 200-400 word long, and
are authored by native speakers of five Germanic languages (German,
Swiss German, Dutch, Swedish and Danish) and five Romance languages
(French, Italian, Catalan, Spanish and Portuguese). Again, SVM is the
classification device. Features include POS $n$-grams, character
$n$-grams, phrase-structure rules (extracted from parse trees), and
two measures of error rate. The classifier is evaluated on its ability
to distinguish between pairs of closely-related L1s, and the results
are usually excellent.

A completely different approach is offered by
\citet{brooke2011native}. Since training corpora for this task are
rare, they use mainly L1 (blog) corpora. Given English word bi-grams
$\langle e_1,e_2\rangle$, they try to assess, for each L1, how likely
it is that an L1 bi-gram was translated literally by the author,
resulting in $\langle e_1,e_2\rangle$. Working with four L1s (French,
Spanish, Chinese, and Japanese), and evaluating on the International
Corpus of Learner English, the results are lower than 50\%.

Our dataset in this work is different, and consists of TOEFL essays
written by speakers of eleven different L1s
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR},
distributed as part of the First Native Language Identification Shared
Task \citep{tetreault-blanchard-cahill:2013:BEA}. We use a plethora of
features; some of them are inspired by previous work outlined above,
but many are motivated by other author attribution tasks, in
particular identification of \emph{translationese}, the language of
translated texts \citep{vered:noam:shuly}.

\section{Methodology}
\label{sec:methodology}
Characteristics of the dataset. Development, train, test sets.

For classification we use \emph{creg}...

Pre-processing: POS-tagging, etc.

\section{Model Overview}
%\label{sec:features}

\nss{I think we should break this up into sections by feature group (below). 
Here we can talk about learning and regularization and give a high-level overview 
of features.}

We define a large arsenal of features, our motivation being both to
improve the accuracy of classification and to be able to interpret the
characteristics of the language produced by speakers of different
L1s. In this section we define the features and motivate their use.%
\footnote{Whenever counts are mentioned, we use the log of the count
  as the feature.}  
While some of the features were used in the works surveyed in
\Sref{sec:literature}, many are novel, and are inspired by the
features used to identify translationese by \citet{vered:noam:shuly}.
We also report the accuracy of using each feature type, in isolation,
on the training set.

\begin{compactdesc}
\item[Character $n$-grams] The number of character 1-, 2-, and
  3-grams. 69.94\%.
\item[Frequent character $n$-grams] Only those character $n$-grams
  that are observed more than $m$ times in the corpus are
  considered. ??? \nt{this includes 1 to 4 $n$-grams, resulting in 74.12\%}
\item[POS $n$-grams] All essays were tagged with the Stanford part-of-speech tagger \citep{toutanova-03}.
Features were extracted to count every POS 1-, 2-, and 3-gram in each document. 53.92\%.
\item[Document length] The number of tokens in the text. 11.81\%.
\item[Pronouns] The number of each pronoun. 22.81\%.\nss{subsumed by contextual function words, right?}
\item[Punctuation] The number of each punctuation mark. 27.41\%.\nss{subsumed by char n-grams, right?}
\item[Passives] The ratio of verbs to passive verbs. 12.26\%.
\item[Positional token frequency] The choice of the first and last few
  words in a sentence is highly constrained, and may be significantly
  influenced by the authors L1. We use the counts (???)\yt{counts for first two and last three words before the period} of the first
  and last three words in each sentence as features. 53.03\%.
\item[Cohesive markers] These are 40 function words (and short phrases)
  that have a strong discourse function in texts \nt{such as 'however', 'becuase', 'in fact' etc}, contributing to its
  cohesiveness. 25.71\%. \nt{Translators tend to spell out implicit utterances and render them explicitly in the target text (REFERENCE TO BLUM-KULKA); MAYBE APPEND THE LIST IN THE END?} 
\item[Cohesive verbs] \nt{This is a list of manually compiled verbs that serve, like `cohesive markers' to spell out implicit utterances; they include, among others, `indicating', `implying' and `containing'. Same, consider appending the list}. 22.85\%.
\item[Function words] The number of occurrences of each word from a
  pre-defined list of 100 \nt{did you use only 100? it should be 400 and the reference should be to Koppel and Ordan} \yt{we used 100. differences between 100 and >100 were insignificant} most frequent words in English (excluding
  punctuation). 42.47\%.
\item[Contextual function words, bigrams] Pairs consisting of a
  function word from the list mentioned above, along with the POS tag
  of its adjacent word. This feature captures patterns such as verbs
  and the preposition or particle immediately to their right, or nouns
  and the determiner that precedes them. 62.79\%
\item[Contextual function words, trigrams] Same as above, but counting
  \nt{2- or} 3-grams consisting of one or two function words (respectively) and the POS tag of the third word \nt{character?} in the \nt{2- or} 3-gram. 62.32\%.
\item[Lemmas] \nt{these should be actually `stems' as produced by the Stanford tagger} The number of each of the most frequent lemmas in the
  text. ???. 58.95\%.
\item[Prompt] Conjunction of the character $n$-gram features defined
  above with the prompt; since the prompt contributes information on
  the domain, it is likely that some words (and, hence, character
  sequences) will occur more frequently with some prompts than with
  others. 65.09\%.
\item[Misspelling features] ???. 37.29\%.
\item[Brown] ???
\item[Restored] \yt{Counts of substitutions, deletions and insertions of predefined tags that we restored in essays edited with a spelling  corrector and missing all these tags. We define three types of tags: (1) punctuation marks (same list as for punctuation feature), (2) function words (WH-words, prepositions, conjunctions, articles, auxiliary verbs, quantifiers, personal pronouns, possessive pronouns, quantified pronouns), (3) cohesion verbs (lemma, present contiguous/progressive and present simple for each verb). We first remove these tags from texts and then recover the most likely hidden tags in a sequence of words, according to an N-gram language model trained on all essays in the training corpus corrected with a spell checker and containing both words and hidden tags. This feature should capture specific words or punctuation marks that are consistently omitted (deletions), or misused (insertions, substitutions).  To restore hidden tags we use hidden-ngram utility provided in SRILM toolkit\footnote{http://www.speech.sri.com/projects/srilm/manpages/hidden-ngram.1.html} (ref to SRILM). 47.67\%}


\end{compactdesc}

\section{Main Features}\label{sec:mainfeats}

These four feature groups form the core of our model.

\subsection{\feat{POS}: part-of-speech sequences}

\subsection{\feat{FreqChar}: frequent character $n$-grams}

\subsection{\feat{CharPrompt}: character $n$-grams paired with the prompt ID}

\subsection{\feat{Brown}: Brown clusters}

\section{Additional Features}

Each of these adds a small number of parameters to the model. 
We report the empirical improvement that each of these brings 
independently when added to the main features (\Sref{sec:mainfeats}).
The full model combines all features.

\subsection{\feat{CxtFxn}: Contextual function words}

\subsection{\feat{DocLen}: Document length in tokens}

\subsection{\feat{Misspell}: Spelling correction edits}

\subsection{\feat{Position}: \nss{?}}

\subsection{\feat{Pron}: pronouns\nss{?}} \nt{A list of 25 pronouns in English, again, append list}

\subsection{\feat{PsvRatio}: Ratio of passive to active voice verbs\nss{or is it the proportion of passive verbs?}}

\subsection{\feat{Punct}: Count of each punctuation mark}

\subsection{\feat{Restore}: LM-restored function words}

\subsection{Discarded Features}

\nss{things that didn't help in our preliminary experiments}

\section{Results}
\label{sec:results}

\begin{table}\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\feat{POS} & 540,947 & 55.18 & 1.0 \\
+ \feat{FreqChar} & 1,036,871 & 79.55 & 1.0 \\ 
\quad + \feat{CharPrompt} & 2,111,175 & 79.82 & 1.0 \\ 
\qquad + \feat{Brown} & 5,664,461 & 81.09 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} feature groups, added cumulatively. 
The number of parameters is always a multiple of 11 (the number of classes). 
Only $\ell_2$ regularization was used for these experiments; 
the penalty was tuned on the dev set as well.}\label{tbl:mainfeats}
\end{table}

\begin{table}\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\textsc{main} + \feat{Position} & 6,153,015 & 81.00 & 1.0 \\
\textsc{main} + \feat{PsvRatio} & 5,664,472 & 81.00 & 1.0 \\
\textsc{main} & 5,664,461 & 81.09 & 1.0 \\
\textsc{main} + \feat{DocLen} & 5,664,472 & 81.09 & 1.0 \\
\textsc{main} + \feat{Pron} & 5,664,736 & 81.09 & 1.0 \\
\textsc{main} + \feat{Punct} & 5,664,604 & 81.09 & 1.0 \\
\textsc{main} + \feat{Misspell} & 5,799,860 & 81.27 & 5.0 \\
\textsc{main} + \feat{Restore} & 5,682,589 & 81.36 & 5.0 \\
\textsc{main} + \feat{CxtFxn} & 7,669,684 & 81.73 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} features plus additional feature groups, added independently. 
$\ell_2$ regularization was tuned as in \tref{tbl:mainfeats} (two values, 1.0 and 5.0, were tried for each 
configuration; more careful tuning might produce slightly better accuracy).
Results are sorted by accuracy; only three groups exhibited independent improvements over the \textsc{main} feature set.}\label{tbl:addfeats}
\end{table}

\begin{table*}\small\centering
\begin{tabular}{>{\bf}l|r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r|ccc} %from the email sent by the organizers
	& \bf ARA & \bf CHI & \bf FRE & \bf GER & \bf HIN & \bf ITA & \bf JPN & \bf KOR & \bf SPA & \bf TEL & \bf TUR & \bf Precision (\%) & \bf Recall (\%) & \bf $F_1$ (\%) \\
\hline
ARA & 80 & 0 & 2 & 1 & 3 & 4 & 1 & 0 & 4 & 2 & 3 & 80.8 & 80.0 & 80.4 \\
CHI & 3 & 80 & 0 & 1 & 1 & 0 & 6 & 7 & 1 & 0 & 1 & 88.9 & 80.0 & 84.2 \\
FRE & 2 & 2 & 81 & 5 & 1 & 2 & 1 & 0 & 3 & 0 & 3 & 86.2 & 81.0 & 83.5 \\
\hline
GER & 1 & 1 & 1 & 93 & 0 & 0 & 0 & 1 & 1 & 0 & 2 & 87.7 & 93.0 & 90.3 \\
HIN & 2 & 0 & 0 & 1 & 77 & 1 & 0 & 1 & 5 & 9 & 4 & 74.8 & 77.0 & 75.9 \\
ITA & 2 & 0 & 3 & 1 & 1 & 87 & 1 & 0 & 3 & 0 & 2 & 82.1 & 87.0 & 84.5 \\
\hline
JPN & 2 & 1 & 1 & 2 & 0 & 1 & 87 & 5 & 0 & 0 & 1 & 78.4 & 87.0 & 82.5 \\
KOR & 1 & 5 & 2 & 0 & 1 & 0 & 9 & 81 & 1 & 0 & 0 & 80.2 & 81.0 & 80.6 \\
SPA & 2 & 0 & 2 & 0 & 1 & 8 & 2 & 1 & 78 & 1 & 5 & 77.2 & 78.0 & 77.6 \\
\hline
TEL & 0 & 1 & 0 & 0 & 18 & 1 & 2 & 1 & 1 & 73 & 3 & 85.9 & 73.0 & 78.9 \\
TUR & 4 & 0 & 2 & 2 & 0 & 2 & 2 & 4 & 4 & 0 & 80 & 76.9 & 80.0 & 78.4 \\
\end{tabular}
\caption{Official test set confusion matrix with the full model. \nss{which direction is predicted vs. gold?}
The largest class of errors was predicting Telugu where the correct label was Hindi\nss{or vice versa?}---this happened 18 times. 
Accuracy is 81.5\%.}\label{tbl:matrix}
\end{table*}


\section{Conclusion}
\label{sec:conclusion}

\section*{Acknowledgments}
This research was supported by a grant from the Israeli Ministry of
Science and Technology.\nss{anything from the CMU side?}

\clearpage

\bibliography{l1id}
\bibliographystyle{plainnat}

\end{document}
