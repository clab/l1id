\documentclass[11pt,letterpaper]{article}
\usepackage[linkcolor=blue]{hyperref}	% must precede ACL style
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}


\usepackage{amsmath,amsfonts,eucal,amsbsy,amsthm,amsopn,amssymb}

\usepackage{txfonts}
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}

\usepackage{paralist}
\defaultleftmargin{1em}{}{}{} % default is 2em

\usepackage{array}

\usepackage[round]{natbib}
\setlength\titlebox{6.5cm}    % Expanding the titlebox



% Author comments
\usepackage{color}
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{purple}{rgb}{0.7,0,1.0}
\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\swmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{S}}_{\textsc{W}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ntmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{N}}_{\textsc{T}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\sw}[1]{\arkcomment{\swmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\nt}[1]{\arkcomment{\ntmarker}{#1}{orange}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\cost}{c}

\newcommand{\Sref}[1]{\S\ref{#1}}
\newcommand{\fref}[1]{figure~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{table~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}


% special macros

\newcommand{\feat}[1]{\textsmaller[.5]{\textsf{#1}}} % code for a feature group




\title{Identifying the L1 of non-native writers: the CMU-Haifa system\\[1em]
{\large Chris Dyer$^\ast$ Manaal Faruqui$^\ast$ Noam Ordan$^\dagger$ Nathan Schneider$^\ast$\\ Yulia Tsvetkov$^\ast$ Naama Twitto$^\dagger$ Shuly Wintner$^\dagger$}\\[-3em]
}

\author{
\\
$^\ast$Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
\And
\\
$^\dagger$Department of Computer Science\\University of Haifa\\Haifa, Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%C.\ Dyer M.\ Faruqui N.\ Schneider Y.\ Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%N.\ Ordan N.\ Twitto S.\ Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
%
%
%Chris Dyer\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{cdyer@cs.cmu.edu}
%\And
%Manaal Faruqui\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{manaalfar@gmail.com}
%\And
%Noam Ordan\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{noam.ordan@gmail.com}
%\And
%Nathan Schneider\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{nathan@cmu.edu}
%\And
%Yulia Tsvetkov\\Language Technologies Institute\\Carnegie Mellon University\\Pittsburgh, PA\\\texttt{yulia.tsvetkov@gmail.com}
%\And
%Naama Twitto\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{naama.twitto@gmail.com}
%\And
%Shuly Wintner\\Department of Computer Science\\University of Haifa\\Israel\\\texttt{shuly@cs.haifa.ac.il}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Given a dataset of English essays composed by non-native speakers, as
part of the TOEFL exam, we identify with high accuracy the native
language of the authors. We use standard text classification
techniques, but define sophisticated classifiers that are sensitive to
the specific patterns observed in the English of authors whose first
language is structurally different. We describe the various features
used for classification, as well as the settings of the classifier
that yielded the highest accuracy.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The task we address in this work is identifying the native language
(\emph{L1}) of non-native English authors. More specifically, given a
dataset of short English essays
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR},
composed as part of the \emph{Test of English as a Foreign Language
  (TOEFL)} by authors whose native language is one out of~11 possible
languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu, and Turkish), our task is to identify that
language.

This task has a clear empirical motivation. Non-native speakers make
different errors when they write English, depending on their native
language \citep{swan2001learner}; understanding the different types of
errors is a prerequisite for correcting them
\citep{Leacock:2010:AGE:1855062}, and systems such as the one we
describe here can shed interesting light on such errors. Tutoring
applications can use our system to identify the native language of
students and offer better-targeted advice. Forensic linguistic
applications are sometimes required to determine the L1 of authors
\citep{estival-gaustad-pham-radford-hutchinson:2007:ALTA2007,estival2007author}. Additionally,
we believe that the task is interesting in and of itself, providing a
better understanding of non-native language. We are thus equally
interested in defining \emph{meaningful} features whose contribution
to the task can be linguistically interpreted.

We address the task as a multiway text-classification task; we specify
our methodology in \Sref{sec:methodology}. As in other author
attribution tasks \citep{joula:2006}, the choice of features for the
classifier is crucial; we discuss the features we define in
\Sref{sec:features}. We report our results in
\Sref{sec:results} and conclude with suggestions for future
research.

\section{Related work}
\label{sec:literature}
The task of L1 identification was introduced by
\citet{koppel2005automatically,koppel2005determining}, who work on the
International Corpus of Learner English \citep{icle}, which includes
texts written by students from Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts lengths range from 500 to 850 words. The
classification method is a linear SVM, and features include 400
standard function words, 200 letter $n$-grams, 185 error types and 250
rare part-of-speech (POS) bi-grams. Ten-fold cross-validation results
on this dataset are 80\% accuracy.

The same experimental setup is assumed by
\citet{tsur-rappoport:2007:Cognitive-2007}, who are mostly interested
in testing the hypothesis that an author's choice of words in a second
language is influenced by the phonology of his or her L1. They confirm
this hypothesis by carefully analyzing the features used by
\citeauthor{koppel2005automatically}, controlling for potential
biases.

\citet{Wong-Dras:2009:ALTA2009,wong-dras:2011:EMNLP} are also
motivated by a linguistic hypothesis, namely that syntactic errors in
a text are influenced by the author's
L1. \citet{Wong-Dras:2009:ALTA2009} analyze three error types
statistically, and then add them as features in the same experimental
setup as above (using LIBSVM with a radial kernel for
classification). The error types are subject-verb disagreement,
noun-number disagreement and misuse of determiners. Addition of these
features does not improve on the results of
\citeauthor{koppel2005automatically}. \citet{wong-dras:2011:EMNLP}
further extend this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic structure. This improves
the results significantly, yielding 78\% accuracy compared with less
than 65\% using only lexical features.

\citet{kochmar2011identification} uses a different corpus, the
Cambridge Learner Corpus, in which texts are 200-400 word long, and
are authored by native speakers of five Germanic languages (German,
Swiss German, Dutch, Swedish and Danish) and five Romance languages
(French, Italian, Catalan, Spanish and Portuguese). Again, SVM is the
classification device. Features include POS $n$-grams, character
$n$-grams, phrase-structure rules (extracted from parse trees), and
two measures of error rate. The classifier is evaluated on its ability
to distinguish between pairs of closely-related L1s, and the results
are usually excellent.

A completely different approach is offered by
\citet{brooke2011native}. Since training corpora for this task are
rare, they use mainly L1 (blog) corpora. Given English word bi-grams
$\langle e_1,e_2\rangle$, they try to assess, for each L1, how likely
it is that an L1 bi-gram was translated literally by the author,
resulting in $\langle e_1,e_2\rangle$. Working with four L1s (French,
Spanish, Chinese, and Japanese), and evaluating on the International
Corpus of Learner English, the results are lower than 50\%.

Our dataset in this work is different, and consists of TOEFL essays
written by speakers of eleven different L1s
\citep{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR},
distributed as part of the First Native Language Identification Shared
Task \citep{tetreault-blanchard-cahill:2013:BEA}. We use a plethora of
features; some of them are inspired by previous work outlined above,
but many are motivated by other author attribution tasks, in
particular identification of \emph{translationese}, the language of
translated texts \citep{vered:noam:shuly}.

\section{Methodology}
\label{sec:methodology}
Characteristics of the dataset. Development, train, test sets.

For classification we use \emph{creg}... \nss{Here we can talk about
  learning and regularization and give a high-level overview of
  features.}

All essays were tagged with the Stanford part-of-speech tagger
\citep{toutanova-03}. We did not parse the dataset.

\section{Model Overview}
\label{sec:features}
We define a large arsenal of features, our motivation being both to
improve the accuracy of classification and to be able to interpret the
characteristics of the language produced by speakers of different
L1s.

\subsection{Motivation}
While some of the features were used in the works surveyed in
\Sref{sec:literature}, many are novel, and are inspired by the
features used to identify translationese by \citet{vered:noam:shuly}.
We begin by motivating our choice of features.

\begin{compactdesc}
\item[POS $n$-grams] Part-of-speech $n$-grams were used in various
  text-classification tasks.
\item[Prompt] Since the prompt contributes information on the domain,
  it is likely that some words (and, hence, character sequences) will
  occur more frequently with some prompts than with others. We
  therefore use the prompt ID in conjunction with other features.
\item[Document length] The number of tokens in the text is highly
  correlated with the author's level of fluency, which in turn is
  correlated with the author's L1.
\item[Pronouns] The use of pronoun varies greatly among different
  authors. We use the same list of~25 English pronouns that
  \citet{vered:noam:shuly} use for identifying translationese.
\item[Punctuation] Similarly, different languages use punctuation
  differently, and we expect this to taint the use of punctuation in
  non-native texts.
\item[Passives] English uses passive voice more frequently than other
  languages. Again, the use of passives in L2 can be correlated with
  the author's L1.
\item[Positional token frequency] The choice of the first and last few
  words in a sentence is highly constrained, and may be significantly
  influenced by the author's L1.
\item[Cohesive markers] These are 40 function words (and short
  phrases) that have a strong discourse function in texts (`however',
  `because', `in fact', etc.) Translators tend to spell out implicit
  utterances and render them explicitly in the target text
  \citep{Blum-Kulka:1986}. We use the list
  of~\citet{vered:noam:shuly}.
\item[Cohesive verbs] This is a list of manually compiled verbs that
  are used, like cohesive markers, to spell out implicit utterances
  (`indicate', `imply', `contain', etc.)
\item[Function words] Frequent tokens, which are mostly function
  words, have been used successfully for various text classification
  tasks. \citet{koppel-ordan:2011:ACL-HLT2011} define a list of 400
  such words, of which we only use~100 (using the entire list was not
  significantly different).
\item[Contextual function words] To further capitalize on the ability
  of function words to discriminate, we define pairs consisting of a
  function word from the list mentioned above, along with the POS tag
  of its adjacent word. This feature captures patterns such as verbs
  and the preposition or particle immediately to their right, or nouns
  and the determiner that precedes them. We also define 3-grams
  consisting of one or two function words and the POS tag of the third
  word in the 3-gram.
\item[Lemmas] The content of the text is not considered a good
  indication of the author's L1, but many text categorization tasks
  use lemmas (more precisely, the stems produced by the tagger) as
  features approximating the content.
\item[Misspelling features] Clearly, the spelling errors that learners
  make in English depend on the phonological properties of their
  L1. \sw{???}
\item[Restored tags] We focus on three important token classes defined
  above: punctuation marks, function words and cohesive verbs. We
  first remove words in these classes from the texts, and then recover
  the most likely hidden tokens \yt{To Shuly: I removed 'POS'. We recover words, not POS tags, I used the word 'tags' to distinguish between words to recover and all other words. Probably the word 'tag' is misleading, I changed it to 'token'} in a sequence of words, according to
  an $n$-gram language model trained on all essays in the training
  corpus corrected with a spell checker and containing both words and
  hidden tokens. This feature should capture specific words or
  punctuation marks that are consistently omitted (deletions), or
  misused (insertions, substitutions). To restore hidden tokens we use
  the \texttt{hidden-ngram} utility provided in SRILM
  \citep{stolcke02srilm}.
\item[Brown clusters] \sw{???}
\end{compactdesc}

\subsection{Main Features}
\label{sec:mainfeats}
First, we use the following four feature types as the core of our
model.  Whenever counts are mentioned, we use the log of the count as
the feature.  We report the accuracy of using each feature type, in
isolation, on the training set.

\begin{compactdesc}
\item[\feat{POS}] Part-of-speech $n$-grams.  Features were extracted
  to count every POS 1-, 2-, and 3-gram in each
  document. 53.92\%. \sw{But the table says 55.18}
\item[\feat{FreqChar}] Frequent character $n$-grams.  We experimented
  with character $n$-grams: The number of character 1-, 2-, and
  3-grams. This yielded 69.94\% accuracy.  We then refined the feature
  to include only those character $n$-grams that are observed more
  than $m$ times in the corpus are considered. \sw{$n$ ranges from~1
    to~4, and $m$ is set to~???. 74.12\%}
\item[\feat{CharPrompt}] Conjunction of the character $n$-gram
  features defined above with the prompt ID. 65.09\%.
\item[\feat{Brown}] Brown clusters. \sw{???}
\end{compactdesc}
\noindent
The accuracy of the classifier on the development set using these four
feature types is reported in \Tref{tbl:mainfeats}.

\begin{table}[hbt]
\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\feat{POS} & 540,947 & 55.18 & 1.0 \\
+ \feat{FreqChar} & 1,036,871 & 79.55 & 1.0 \\ 
\quad + \feat{CharPrompt} & 2,111,175 & 79.82 & 1.0 \\ 
\qquad + \feat{Brown} & 5,664,461 & 81.09 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} feature groups, added cumulatively. 
  The number of parameters is always a multiple of 11 (the number of classes). 
  Only $\ell_2$ regularization was used for these experiments; 
  the penalty was tuned on the dev set as well.}
\label{tbl:mainfeats}
\end{table}

\subsection{Additional Features}
To the basic set of features we now add more specific,
linguistically-motivated features, each adding a small number of
parameters to the model.  As above, we indicate the accuracy of each
feature type in isolation.

\begin{compactdesc}
\item[\feat{DocLen}] Document length in tokens. 11.81\%.
\item[\feat{Punct}] Counts of each punctuation mark. 27.41\%.
\item[\feat{Pron}] Counts of each pronoun. 22.81\%.
\item[\feat{Position}] Positional token frequency. We use the counts
  for the first two and last three words before the period in each
  sentence as features. 53.03\%.
\item[\feat{PsvRatio}] The proportion of passive verbs out of all
  verbs. 12.26\%.
\item[\feat{CxtFxn}] Contextual function words. Bi-grams yield
  62.79\%, tri-gram 62.32\%.
\item[\feat{Misspell}] Spelling correction edits. \sw{???}. 37.29\%.
\item[\feat{Restore}] Counts of substitutions, deletions and
  insertions of predefined tokens that we restored in the texts. 47.67\%
\end{compactdesc}
\noindent
\Tref{tbl:addfeats} reports the empirical improvement that each of
these brings independently when added to the main features
(\Sref{sec:mainfeats}).

\begin{table}[hbt]
\small\centering
\begin{tabular}{lrcc}
\textbf{Feature Group} & \multicolumn{1}{c}{\textbf{\# Params}} & \textbf{Accuracy (\%)} & \textbf{$\ell_2$} \\
\hline
\textsc{main} + \feat{Position} & 6,153,015 & 81.00 & 1.0 \\
\textsc{main} + \feat{PsvRatio} & 5,664,472 & 81.00 & 1.0 \\
\textsc{main} & 5,664,461 & 81.09 & 1.0 \\
\textsc{main} + \feat{DocLen} & 5,664,472 & 81.09 & 1.0 \\
\textsc{main} + \feat{Pron} & 5,664,736 & 81.09 & 1.0 \\
\textsc{main} + \feat{Punct} & 5,664,604 & 81.09 & 1.0 \\
\textsc{main} + \feat{Misspell} & 5,799,860 & 81.27 & 5.0 \\
\textsc{main} + \feat{Restore} & 5,682,589 & 81.36 & 5.0 \\
\textsc{main} + \feat{CxtFxn} & 7,669,684 & 81.73 & 1.0 \\
\end{tabular}
\caption{Dev set accuracy with \textsc{main} features plus additional feature groups, added independently. 
$\ell_2$ regularization was tuned as in \Tref{tbl:mainfeats} (two values, 1.0 and 5.0, were tried for each 
configuration; more careful tuning might produce slightly better accuracy).
Results are sorted by accuracy; only three groups exhibited independent improvements over the \textsc{main} feature set.}
\label{tbl:addfeats}
\end{table}


\subsection{Discarded Features}
We also tried several other feature types that did not improve the
accuracy of the classifier on the development set.
\begin{compactdesc}
\item[Cohesive markers] Counts of each cohesive marker. 25.71\%.
\item[Cohesive verbs] Counts of each cohesive verb. 22.85\%.
\item[Function words] Counts of function words. 42.47\%. This feature
  is subsumed by the highly discriminative \feat{CxtFxn} feature.
\end{compactdesc}


\section{Results}
\label{sec:results}
The full model that we used to classify the test set combines all
features listed in \Tref{tbl:addfeats}. Using all these features, the
accuracy on the development set is \sw{???}, and on the test set it
is~81.5\%. \Tref{tbl:matrix} lists the confusion matrix on the test
set, as well as precision, recall and $F_1$-score for each L1.


\begin{table*}[hbt]
\small\centering
\begin{tabular}{>{\bf}l|r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r@{ }r|ccc} %from the email sent by the organizers
	& \bf ARA & \bf CHI & \bf FRE & \bf GER & \bf HIN & \bf ITA & \bf JPN & \bf KOR & \bf SPA & \bf TEL & \bf TUR & \bf Precision (\%) & \bf Recall (\%) & \bf $F_1$ (\%) \\
\hline
ARA & 80 & 0 & 2 & 1 & 3 & 4 & 1 & 0 & 4 & 2 & 3 & 80.8 & 80.0 & 80.4 \\
CHI & 3 & 80 & 0 & 1 & 1 & 0 & 6 & 7 & 1 & 0 & 1 & 88.9 & 80.0 & 84.2 \\
FRE & 2 & 2 & 81 & 5 & 1 & 2 & 1 & 0 & 3 & 0 & 3 & 86.2 & 81.0 & 83.5 \\
\hline
GER & 1 & 1 & 1 & 93 & 0 & 0 & 0 & 1 & 1 & 0 & 2 & 87.7 & 93.0 & 90.3 \\
HIN & 2 & 0 & 0 & 1 & 77 & 1 & 0 & 1 & 5 & 9 & 4 & 74.8 & 77.0 & 75.9 \\
ITA & 2 & 0 & 3 & 1 & 1 & 87 & 1 & 0 & 3 & 0 & 2 & 82.1 & 87.0 & 84.5 \\
\hline
JPN & 2 & 1 & 1 & 2 & 0 & 1 & 87 & 5 & 0 & 0 & 1 & 78.4 & 87.0 & 82.5 \\
KOR & 1 & 5 & 2 & 0 & 1 & 0 & 9 & 81 & 1 & 0 & 0 & 80.2 & 81.0 & 80.6 \\
SPA & 2 & 0 & 2 & 0 & 1 & 8 & 2 & 1 & 78 & 1 & 5 & 77.2 & 78.0 & 77.6 \\
\hline
TEL & 0 & 1 & 0 & 0 & 18 & 1 & 2 & 1 & 1 & 73 & 3 & 85.9 & 73.0 & 78.9 \\
TUR & 4 & 0 & 2 & 2 & 0 & 2 & 2 & 4 & 4 & 0 & 80 & 76.9 & 80.0 & 78.4 \\
\end{tabular}
\caption{Official test set confusion matrix with the full model. \nss{which direction is predicted vs. gold?}
Accuracy is 81.5\%.}
\label{tbl:matrix}
\end{table*}

\sw{Analysis? Error analysis? Observations?}

\section{Conclusion}
\label{sec:conclusion}

\section*{Acknowledgments}
This research was supported by a grant from the Israeli Ministry of
Science and Technology.\nss{anything from the CMU side?}

%\clearpage

\bibliography{l1id}
\bibliographystyle{plainnat}

\end{document}
